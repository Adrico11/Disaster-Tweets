{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import IPython\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Preprocess datasets**\n",
    "\n",
    "- Import BERT Tokenizer + add tokens to mask URLs and usernames\n",
    "- Basic data preprocessing : get rid of tags, links and usernames\n",
    "- Bert preprocessing : tokenize, create inputs and attention masks\n",
    "- Form train and test datasets (in the correct format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[LINK]', '[USER]']})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "## Adding additional tokens for masking URLs and usernames in tweets\n",
    "bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\n",
    "bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(df, tokenizer=bert_tokenizer, max_seq_len = 100):\n",
    "    input_sequences = []\n",
    "    # The attention mask is an optional argument used when batching sequences together.\n",
    "    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n",
    "    attention_masks = []\n",
    "    bert_text = []\n",
    "    \n",
    "    # some very minor text processing, try to keep the text as close as original\n",
    "    for i, text in enumerate(df['text']):\n",
    "#         print(i, text)\n",
    "        text = text.replace(\"\\n\", \" \").split(\" \")\n",
    "        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n",
    "        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n",
    "        text = \" \".join(text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        bert_text.append(text)\n",
    "        \n",
    "#         print(i, text)\n",
    "        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        input_ids = sequence_dict['input_ids']\n",
    "        att_mask = sequence_dict['attention_mask']\n",
    "#         print(i, tokenizer.tokenize(text))\n",
    "        input_sequences.append(input_ids)\n",
    "        attention_masks.append(att_mask)\n",
    "    \n",
    "    df['bert_text'] = bert_text\n",
    "    return input_sequences, attention_masks, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\tranformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 8372, 2089, 16455, 9641, 2149, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2074, 3047, 1037, 6659, 2482, 5823, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_X, train_att, train_df = bert_tokenize(train_df)\n",
    "train_y = train_df['target'].values\n",
    "test_X, test_att, test_df = bert_tokenize(test_df)\n",
    "\n",
    "# Checking the tokenized format\n",
    "print(train_X[0])\n",
    "print(train_att[0])\n",
    "print(test_X[0])\n",
    "print(test_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forming the datasets\n",
    "train_X = torch.tensor(train_X)\n",
    "train_y = torch.tensor(train_y)\n",
    "train_att = torch.tensor(train_att)\n",
    "test_X = torch.tensor(test_X)\n",
    "test_att = torch.tensor(test_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_X, test_att)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_data)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Adapt and Train Bert model**\n",
    "\n",
    "- Resize token embeddings (since we have added two special ones)\n",
    "- Define device on which the training will take place (cuda or cpu)\n",
    "- Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30524, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.resize_token_embeddings(len(bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fct = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Writer will output to ./runs/ directory by default\n",
    "# writer = SummaryWriter(logdir)\n",
    "\n",
    "# training loop\n",
    "# train_loss += loss_train.item()\n",
    "# writer.add_scalar('Loss/train', training_loss, global_step)\n",
    "\n",
    "# testing\n",
    "# val_loss += loss_val.item()\n",
    "# writer.add_scalar('Loss/val', val_loss, global_step)\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train(epoch):\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    t0 = datetime.now()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, att_masks, labels = batch\n",
    "        #print(batch)\n",
    "        model.zero_grad()  \n",
    "        \n",
    "        logits = model(inputs, attention_mask=att_masks)\n",
    "        outputs = F.log_softmax(logits[0], dim=1)\n",
    "        #print(outputs)\n",
    "        \n",
    "        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        pred_outputs = outputs.detach().cpu().numpy()\n",
    "        #print(pred_outputs)\n",
    "        pred = np.argmax(pred_outputs, axis=1)\n",
    "        labels = labels.cpu().numpy()\n",
    "        acc_list.append(accuracy_score(pred, labels))\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}'.format(\n",
    "                epoch, i * len(inputs), len(train_dataloader.dataset),\n",
    "                    i / len(train_dataloader), datetime.now() - t0, loss.item()\n",
    "            ))\n",
    "    return 100*mean(acc_list), mean(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    t0 = datetime.now()\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, att_masks, labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs, attention_mask=att_masks)\n",
    "            outputs = F.log_softmax(logits[0], dim=1)\n",
    "            \n",
    "            loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "        pred = np.argmax(outputs, axis=1)\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        test_acc += accuracy_score(pred, labels)\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n",
    "        test_loss, test_acc, datetime.now() - t0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [640/7613 (8%)] - Elapsed: 0:09:37.155162  |  Loss: 0.5155\n",
      "Train Epoch: 1 [1280/7613 (17%)] - Elapsed: 0:19:02.228424  |  Loss: 0.4239\n",
      "Train Epoch: 1 [1920/7613 (25%)] - Elapsed: 19:10:11.476495  |  Loss: 0.4824\n",
      "Train Epoch: 1 [2560/7613 (34%)] - Elapsed: 19:15:30.340208  |  Loss: 0.5009\n",
      "Train Epoch: 1 [3200/7613 (42%)] - Elapsed: 19:20:00.842540  |  Loss: 0.4001\n",
      "Train Epoch: 1 [3840/7613 (50%)] - Elapsed: 19:24:26.796690  |  Loss: 0.5613\n",
      "Train Epoch: 1 [4480/7613 (59%)] - Elapsed: 19:28:44.719283  |  Loss: 0.4545\n",
      "Train Epoch: 1 [5120/7613 (67%)] - Elapsed: 19:33:00.047386  |  Loss: 0.4600\n",
      "Train Epoch: 1 [5760/7613 (76%)] - Elapsed: 19:37:16.013321  |  Loss: 0.5077\n",
      "Train Epoch: 1 [6400/7613 (84%)] - Elapsed: 19:41:33.417413  |  Loss: 0.2726\n",
      "Train Epoch: 1 [7040/7613 (92%)] - Elapsed: 19:45:49.457083  |  Loss: 0.2937\n",
      "Training time per epoch :\n",
      "[(1, 71378)]\n",
      "Total training time for 1 epochs : 1189.63 minutes\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model_name = 'network'\n",
    "log_name = '{}_{}'.format(model_name, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "writer = SummaryWriter('logs/{}'.format(log_name))\n",
    "\n",
    "nb_epoch = 1\n",
    "\n",
    "time_list = []\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    t0 = datetime.now()\n",
    "    accuracy, avg_loss = train(epoch)\n",
    "    time_list.append((datetime.now()-t0).seconds)\n",
    "    writer.add_scalar('loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('acc/train', accuracy, epoch)\n",
    "print(\"Training time per epoch :\")\n",
    "print(list(zip(range(1,nb_epoch+1),time_list)))\n",
    "total_time = round(sum(time_list)/60,2)\n",
    "print(f\"Total training time for {nb_epoch} epochs : {total_time} minutes\")\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=range(len(acc_list))\n",
    "# plt.plot(epochs, acc_list, 'r', 'Training F1')\n",
    "# plt.plot(epochs, val_f1, 'b', 'Validation F1')\n",
    "# plt.title('Training and validation F1')\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss_list, 'r', 'Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Predict on Test data**\n",
    "\n",
    "- Define predict function\n",
    "- Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # pre-process text\n",
    "    input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n",
    "    logits = model.eval()(input_ids=input_)[0]\n",
    "    pred = F.softmax(logits, dim=1)[0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text in test_df.text:\n",
    "    prob = predict(text)\n",
    "    pred = np.argmax(prob.cpu().detach().numpy())\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "# bert\n",
    "sample_submission[\"target\"] = predictions\n",
    "pd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd7f66c493c3378b00f1c25d794bdfda53f41f0227b6475fe6cc86775751149d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tranformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
