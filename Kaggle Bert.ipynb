{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import IPython\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Preprocess datasets**\n",
    "\n",
    "- Import BERT Tokenizer + add tokens to mask URLs and usernames\n",
    "- Basic data preprocessing : get rid of tags, links and usernames\n",
    "- Bert preprocessing : tokenize, create inputs and attention masks\n",
    "- Form train and test datasets (in the correct format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[LINK]', '[USER]']})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "## Adding additional tokens for masking URLs and usernames in tweets\n",
    "bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\n",
    "bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(df, tokenizer=bert_tokenizer, max_seq_len = 100):\n",
    "    input_sequences = []\n",
    "    # The attention mask is an optional argument used when batching sequences together.\n",
    "    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n",
    "    attention_masks = []\n",
    "    bert_text = []\n",
    "    \n",
    "    # some very minor text processing, try to keep the text as close as original\n",
    "    for i, text in enumerate(df['text']):\n",
    "#         print(i, text)\n",
    "        text = text.replace(\"\\n\", \" \").split(\" \")\n",
    "        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n",
    "        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n",
    "        text = \" \".join(text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        bert_text.append(text)\n",
    "        \n",
    "#         print(i, text)\n",
    "        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        input_ids = sequence_dict['input_ids']\n",
    "        att_mask = sequence_dict['attention_mask']\n",
    "#         print(i, tokenizer.tokenize(text))\n",
    "        input_sequences.append(input_ids)\n",
    "        attention_masks.append(att_mask)\n",
    "    \n",
    "    df['bert_text'] = bert_text\n",
    "    return input_sequences, attention_masks, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\tranformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 8372, 2089, 16455, 9641, 2149, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2074, 3047, 1037, 6659, 2482, 5823, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_X, train_att, train_df = bert_tokenize(train_df)\n",
    "train_y = train_df['target'].values\n",
    "test_X, test_att, test_df = bert_tokenize(test_df)\n",
    "\n",
    "# Checking the tokenized format\n",
    "print(train_X[0])\n",
    "print(train_att[0])\n",
    "print(test_X[0])\n",
    "print(test_att[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forming the datasets\n",
    "train_X = torch.tensor(train_X)\n",
    "train_y = torch.tensor(train_y)\n",
    "train_att = torch.tensor(train_att)\n",
    "test_X = torch.tensor(test_X)\n",
    "test_att = torch.tensor(test_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_X, test_att)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_data)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Adapt and Train Bert model**\n",
    "\n",
    "- Resize token embeddings (since we have added two special ones)\n",
    "- Define device on which the training will take place (cuda or cpu)\n",
    "- Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30524, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.resize_token_embeddings(len(bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "loss_fct = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Writer will output to ./runs/ directory by default\n",
    "# writer = SummaryWriter(logdir)\n",
    "\n",
    "# training loop\n",
    "# train_loss += loss_train.item()\n",
    "# writer.add_scalar('Loss/train', training_loss, global_step)\n",
    "\n",
    "# testing\n",
    "# val_loss += loss_val.item()\n",
    "# writer.add_scalar('Loss/val', val_loss, global_step)\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train(epoch):\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    t0 = datetime.now()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, att_masks, labels = batch\n",
    "        model.zero_grad()  \n",
    "        \n",
    "        logits = model(inputs, attention_mask=att_masks)\n",
    "        outputs = F.log_softmax(logits[0], dim=1)\n",
    "        pred = np.argmax(outputs, axis=1)\n",
    "        labels = labels.cpu().numpy()\n",
    "        acc_list.append(accuracy_score(pred, labels))\n",
    "        \n",
    "        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}'.format(\n",
    "                epoch, i * len(inputs), len(train_dataloader.dataset),\n",
    "                    i / len(train_dataloader), datetime.now() - t0, loss.item()\n",
    "            ))\n",
    "    return 100*mean(acc_list), mean(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    t0 = datetime.now()\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, att_masks, labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs, attention_mask=att_masks)\n",
    "            outputs = F.log_softmax(logits[0], dim=1)\n",
    "            \n",
    "            loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "        pred = np.argmax(outputs, axis=1)\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        test_acc += accuracy_score(pred, labels)\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n",
    "        test_loss, test_acc, datetime.now() - t0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32/7613 (0%)] - Elapsed: 0:00:15.195120  |  Loss: 0.6816\n",
      "Train Epoch: 1 [64/7613 (1%)] - Elapsed: 0:00:30.695082  |  Loss: 0.6841\n",
      "Train Epoch: 1 [96/7613 (1%)] - Elapsed: 0:00:46.055452  |  Loss: 0.7100\n",
      "Train Epoch: 1 [128/7613 (2%)] - Elapsed: 0:01:00.768352  |  Loss: 0.6885\n",
      "Train Epoch: 1 [160/7613 (2%)] - Elapsed: 0:01:15.021035  |  Loss: 0.6628\n",
      "Train Epoch: 1 [192/7613 (3%)] - Elapsed: 0:01:29.232949  |  Loss: 0.6646\n",
      "Train Epoch: 1 [224/7613 (3%)] - Elapsed: 0:01:43.340892  |  Loss: 0.6755\n",
      "Train Epoch: 1 [256/7613 (3%)] - Elapsed: 0:01:57.878332  |  Loss: 0.6621\n",
      "Train Epoch: 1 [288/7613 (4%)] - Elapsed: 0:02:13.078232  |  Loss: 0.6440\n",
      "Train Epoch: 1 [320/7613 (4%)] - Elapsed: 0:02:27.198080  |  Loss: 0.6434\n",
      "Train Epoch: 1 [352/7613 (5%)] - Elapsed: 0:02:41.013236  |  Loss: 0.6671\n",
      "Train Epoch: 1 [384/7613 (5%)] - Elapsed: 0:02:55.234149  |  Loss: 0.6465\n",
      "Train Epoch: 1 [416/7613 (5%)] - Elapsed: 0:03:09.159097  |  Loss: 0.6219\n",
      "Train Epoch: 1 [448/7613 (6%)] - Elapsed: 0:03:22.893456  |  Loss: 0.6520\n",
      "Train Epoch: 1 [480/7613 (6%)] - Elapsed: 0:03:36.592332  |  Loss: 0.6348\n",
      "Train Epoch: 1 [512/7613 (7%)] - Elapsed: 0:03:50.750338  |  Loss: 0.6197\n",
      "Train Epoch: 1 [544/7613 (7%)] - Elapsed: 0:04:05.180525  |  Loss: 0.5773\n",
      "Train Epoch: 1 [576/7613 (8%)] - Elapsed: 0:04:19.941229  |  Loss: 0.5519\n",
      "Train Epoch: 1 [608/7613 (8%)] - Elapsed: 0:04:34.144624  |  Loss: 0.5881\n",
      "Train Epoch: 1 [640/7613 (8%)] - Elapsed: 0:04:48.525723  |  Loss: 0.5344\n",
      "Train Epoch: 1 [672/7613 (9%)] - Elapsed: 0:05:02.610518  |  Loss: 0.7144\n",
      "Train Epoch: 1 [704/7613 (9%)] - Elapsed: 0:05:16.487996  |  Loss: 0.5395\n",
      "Train Epoch: 1 [736/7613 (10%)] - Elapsed: 0:05:31.140756  |  Loss: 0.5086\n",
      "Train Epoch: 1 [768/7613 (10%)] - Elapsed: 0:05:45.935292  |  Loss: 0.6147\n",
      "Train Epoch: 1 [800/7613 (11%)] - Elapsed: 0:06:00.608680  |  Loss: 0.5513\n",
      "Train Epoch: 1 [832/7613 (11%)] - Elapsed: 0:06:15.422406  |  Loss: 0.5362\n",
      "Train Epoch: 1 [864/7613 (11%)] - Elapsed: 0:06:30.815391  |  Loss: 0.5909\n",
      "Train Epoch: 1 [896/7613 (12%)] - Elapsed: 0:06:45.988708  |  Loss: 0.5547\n",
      "Train Epoch: 1 [928/7613 (12%)] - Elapsed: 0:07:01.488377  |  Loss: 0.5435\n",
      "Train Epoch: 1 [960/7613 (13%)] - Elapsed: 0:07:16.948578  |  Loss: 0.4961\n",
      "Train Epoch: 1 [992/7613 (13%)] - Elapsed: 0:07:31.571259  |  Loss: 0.4761\n",
      "Train Epoch: 1 [1024/7613 (13%)] - Elapsed: 0:07:45.718429  |  Loss: 0.5666\n",
      "Train Epoch: 1 [1056/7613 (14%)] - Elapsed: 0:08:00.844323  |  Loss: 0.4327\n",
      "Train Epoch: 1 [1088/7613 (14%)] - Elapsed: 0:08:15.652473  |  Loss: 0.5387\n",
      "Train Epoch: 1 [1120/7613 (15%)] - Elapsed: 0:08:30.987777  |  Loss: 0.4075\n",
      "Train Epoch: 1 [1152/7613 (15%)] - Elapsed: 0:08:46.307205  |  Loss: 0.3478\n",
      "Train Epoch: 1 [1184/7613 (16%)] - Elapsed: 0:09:01.153686  |  Loss: 0.3325\n",
      "Train Epoch: 1 [1216/7613 (16%)] - Elapsed: 0:09:15.704738  |  Loss: 0.3298\n",
      "Train Epoch: 1 [1248/7613 (16%)] - Elapsed: 0:09:30.077254  |  Loss: 0.4199\n",
      "Train Epoch: 1 [1280/7613 (17%)] - Elapsed: 0:09:45.251593  |  Loss: 0.4857\n",
      "Train Epoch: 1 [1312/7613 (17%)] - Elapsed: 0:09:59.837794  |  Loss: 0.4134\n",
      "Train Epoch: 1 [1344/7613 (18%)] - Elapsed: 0:10:14.777412  |  Loss: 0.5255\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model_name = 'network'\n",
    "log_name = '{}_{}'.format(model_name, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "writer = SummaryWriter('logs/{}'.format(log_name))\n",
    "\n",
    "nb_epoch = 5\n",
    "\n",
    "time_list = []\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    t0 = datetime.now()\n",
    "    accuracy, avg_loss = train(epoch)\n",
    "    time_list.append(datetime.now()-t0)\n",
    "    writer.add_scalar('loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('acc/train', accuracy, epoch)\n",
    "print(\"Training time per epoch :\")\n",
    "print(list(zip(range(1,nb_epoch+1),time_list)))\n",
    "total_time = round(sum(time_list),2)\n",
    "print(f\"Total training time for {nb_epoch} epochs : {total_time} seconds\")\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=range(len(acc_list))\n",
    "# plt.plot(epochs, acc_list, 'r', 'Training F1')\n",
    "# plt.plot(epochs, val_f1, 'b', 'Validation F1')\n",
    "# plt.title('Training and validation F1')\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss_list, 'r', 'Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Predict on Test data**\n",
    "\n",
    "- Define predict function\n",
    "- Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # pre-process text\n",
    "    input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n",
    "    logits = model.eval()(input_ids=input_)[0]\n",
    "    pred = F.softmax(logits, dim=1)[0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text in test_df.text:\n",
    "    prob = predict(text)\n",
    "    pred = np.argmax(prob.cpu().detach().numpy())\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "# bert\n",
    "sample_submission[\"target\"] = predictions\n",
    "pd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd7f66c493c3378b00f1c25d794bdfda53f41f0227b6475fe6cc86775751149d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tranformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
