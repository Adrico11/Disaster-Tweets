{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CURRENT STANDINGS : 330/860 !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KAGGLE COMPETITION : MACHINE LEARNING APPROACH**\n",
    "\n",
    "**A. Preprocessing**\n",
    "1. Import libraries\n",
    "2. Import dataset\n",
    "3. Basic data cleaning : get rid of null values, punctuation and html tags\n",
    "4. NLP data preprocessing : remove stop words, stem/lemmatize words (+ remove repeating words)\n",
    "5. Test-Train split\n",
    "6. Vecorize data (fit on the train data and use on train+test data) with countvec or tfidfvec\n",
    "7. Oversample data (deal with imbalanced data with SMOTE)\n",
    "\n",
    "**B. Models**\n",
    "Benchmark ML models (LogisticRegression, LinearSVC, Multi-class Naive Bayes, Random forest classifier...)\n",
    "\n",
    "**C. Make prediction** \n",
    "\n",
    "Submit predictions and put my name on the leaderboard !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adrie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import os\n",
    "import nltk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "def clean_reviews(review):\n",
    "    #find and replace all punctuation with \"\" and all html tags with \" \" + lower\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    review = REPLACE_NO_SPACE.sub(\"\", review.lower()) #replace specified characters with \"\" in the review.lower() text\n",
    "    review = REPLACE_WITH_SPACE.sub(\" \", review)\n",
    "    return review\n",
    "\n",
    "# def remove_repeating_words(review):\n",
    "#     review1 = yield(gensim.utils.simple_preprocess(str(review),deacc=True))\n",
    "#     return review1\n",
    "\n",
    "def remove_stop_words(review):\n",
    "    stop_words_list = ENGLISH_STOP_WORDS #stop_words.\n",
    "    review = \" \".join(word for word in review.split(\" \") if word not in stop_words_list)\n",
    "    return review\n",
    "\n",
    "def stem_reviews(review):\n",
    "    # PB : NEED TO INITIATE A NEW STEMMER FOR EACH ROW...\n",
    "    stemmer = PorterStemmer()\n",
    "    #separator.join(list of strings) => 1 long string\n",
    "    #string.split(separator) => list of strings\n",
    "    review = \" \".join([stemmer.stem(word) for word in review.split(\" \")])\n",
    "    return review\n",
    "\n",
    "def lemmatize_reviews(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #separator.join(list of strings) => 1 long string\n",
    "    #string.split(separator) => list of strings\n",
    "    review = \" \".join([lemmatizer.lemmatize(word) for word in review.split(\" \")])\n",
    "    return review\n",
    "\n",
    "def oversample(data_list):\n",
    "    [X_train, y_train, X_eval, y_eval] = data_list\n",
    "    sm = SMOTE()\n",
    "    X_train_sm, y_train_sm = sm.fit_resample(X_train,y_train)\n",
    "    X_eval_sm, y_eval_sm = sm.fit_resample(X_eval,y_eval)\n",
    "    assert len(y_train_sm.value_counts().unique())==1 and len(y_eval_sm.value_counts().unique())==1\n",
    "    return [X_train_sm, y_train_sm,X_eval_sm, y_eval_sm]\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, df, train = True, vec = None, nlp_method = \"lemmatize\", vec_name = \"tfidf\", stop_words = \"english\", ngram_range = (1,2)) -> None: # change vec_name to tfidf to see what changes !!!\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "        self.vec = vec\n",
    "        assert (self.train == True and self.vec == None) or (self.train == False and self.vec != None)\n",
    "        assert nlp_method in [\"None\", \"stem\",\"lemmatize\"]\n",
    "        self.nlp_method = nlp_method\n",
    "        assert vec_name in [\"binary\",\"count\",\"tfidf\"]\n",
    "        self.vec_name = vec_name\n",
    "        self.stop_words = stop_words # USELESS IF REMOVED BEFORE !!! (in the basic cleaning...)\n",
    "        self.ngram_range = ngram_range\n",
    "        if self.train == True : #same as self.vec == None\n",
    "            if self.vec_name == \"tfidf\":\n",
    "                self.vec =  TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=self.ngram_range) #stop_words=self.stop_words,\n",
    "            else:\n",
    "                self.vec = CountVectorizer(binary=self.vec_name == \"binary\", ngram_range=self.ngram_range) # stop_words=self.stop_words,\n",
    "        else:\n",
    "            self.vec = vec\n",
    "        self.train_df = self.test_df = None\n",
    "\n",
    "    def basic_clean(self) -> None:\n",
    "        print(\"   Dropping null values\")\n",
    "        self.df.dropna(inplace = True)\n",
    "        print(\"   Getting rid of punctuation and HTML tags\")\n",
    "        self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : clean_reviews(row))\n",
    "        print(\"   Removing repeating words\")\n",
    "        \n",
    "        # if self.train: #no labels in the test data\n",
    "        #     self.df[\"Sentiments\"] = self.df[\"Sentiments\"].apply(lambda row : 0 if row==\"negative\" else 1)\n",
    "\n",
    "    def nlp_clean(self) -> None:\n",
    "        print(\"   Removing stop words\")\n",
    "        self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : remove_stop_words(row))\n",
    "        if self.nlp_method == \"stem\":\n",
    "            print(\"   Stemming reviews\")\n",
    "            self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : stem_reviews(row))\n",
    "        elif self.nlp_method == \"lemmatize\":\n",
    "            print(\"   Lemmatizing reviews\")\n",
    "            self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : lemmatize_reviews(row))\n",
    "        else:\n",
    "            print(\"   No NLP cleaning\")\n",
    "\n",
    "    def train_eval_split(self, train_size = 0.8, random_state = 42) -> None:\n",
    "        M = self.df.shape[0]\n",
    "        self.train_df = self.df.sample(n=int(train_size*M), axis=0, random_state = random_state, ignore_index=True)\n",
    "        self.eval_df = self.df[~self.df.index.isin(self.train_df.index)].reset_index(drop=True)\n",
    "\n",
    "    def vectorize_train(self):\n",
    "        print(f\"   Vecorizing training data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        train_reviews = self.train_df[\"Reviews\"]\n",
    "        eval_reviews = self.eval_df[\"Reviews\"]\n",
    "        vec.fit(train_reviews)\n",
    "        X_train = vec.transform(train_reviews)\n",
    "        X_eval = vec.transform(eval_reviews)\n",
    "        return vec, X_train, X_eval\n",
    "\n",
    "    def final_split(self):\n",
    "        y_train = self.train_df[\"Sentiments\"]\n",
    "        y_eval = self.eval_df[\"Sentiments\"]\n",
    "        vec, X_train, X_eval = self.vectorize_train()\n",
    "        return vec, [X_train, y_train, X_eval, y_eval]\n",
    "\n",
    "    def vectorize_test(self):\n",
    "        print(f\"   Vecorizing test data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        vec_reviews = vec.transform(self.df[\"Reviews\"])\n",
    "        #full_vec_reviews = PUT EVERYTHING BACK TOGETHER ???\n",
    "        return vec, vec_reviews\n",
    "\n",
    "    def vectorize_full_train(self):\n",
    "        print(f\"   Vecorizing full training data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        X_train_full = vec.fit_transform(self.df[\"Reviews\"])\n",
    "        y_train_full = self.df[\"Sentiments\"]\n",
    "        #full_vec_reviews = PUT EVERYTHING BACK TOGETHER ???\n",
    "        return vec, [X_train_full, y_train_full]\n",
    "\n",
    "def data_preprocessor_train(filename, separator, header, use_cols, col_dict, smote = True):\n",
    "    \"\"\"Preprocess the train dataset (subset of the initial train dataset) to train the ML models in the benchmark phase\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator, usecols = use_cols, header = header)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    print(\"Train-Eval split...\")\n",
    "    df_preprocess.train_eval_split()\n",
    "    train_vec, train_data_list = df_preprocess.final_split()\n",
    "    if smote:\n",
    "        print(\"Balancing dataset...\")\n",
    "        train_data_list = oversample(train_data_list)\n",
    "    return train_vec, train_data_list  #vec, X_train, y_train, X_eval, y_eval\n",
    "\n",
    "def data_preprocessor_test(filename, separator, header, use_cols, index_col, col_dict, vec):\n",
    "    \"\"\"Preprocess the eval dataset (subset of the initial train dataset) to evaluate the ML models in the benchmark phase\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator,header = header, usecols = use_cols, index_col = index_col)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init, train = False, vec = vec)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    test_vec, test_data_list = df_preprocess.vectorize_test()\n",
    "    return test_vec, test_data_list  #vec, X_test, y_test\n",
    "\n",
    "def data_preprocessor_full_train(filename, separator, header, use_cols, col_dict, smote = True):\n",
    "    \"\"\"Preprocess the whole train dataset to make better final predictions\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator, usecols = use_cols, header = header)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(df_preprocess.df)\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    print(df_preprocess.df)\n",
    "    full_train_vec, full_train_data_list = df_preprocess.vectorize_full_train()\n",
    "    print(len(full_train_data_list))\n",
    "    # if smote:\n",
    "    #     print(\"Balancing dataset...\")\n",
    "    #     full_train_data_list = oversample(full_train_data_list)\n",
    "    return full_train_vec, full_train_data_list  #vec, X_train, y_train, X_eval, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* PREPROCESSING TRAIN DATA *******************\n",
      "Reading input data\n",
      "(156060, 2)\n",
      "                                             Reviews  Sentiments\n",
      "0  A series of escapades demonstrating the adage ...           1\n",
      "1  A series of escapades demonstrating the adage ...           2\n",
      "2                                           A series           2\n",
      "3                                                  A           2\n",
      "4                                             series           2\n",
      "Basic cleaning...\n",
      "   Dropping null values\n",
      "   Getting rid of punctuation and HTML tags\n",
      "NLP cleaning...\n",
      "   Removing stop words\n",
      "   Lemmatizing reviews\n",
      "Train-Eval split...\n",
      "   Vecorizing training data : tfidf\n",
      "Balancing dataset...\n",
      "******************* PREPROCESSING FULL TRAIN DATA *******************\n",
      "Reading input data\n",
      "(156060, 2)\n",
      "                                             Reviews  Sentiments\n",
      "0  A series of escapades demonstrating the adage ...           1\n",
      "1  A series of escapades demonstrating the adage ...           2\n",
      "2                                           A series           2\n",
      "3                                                  A           2\n",
      "4                                             series           2\n",
      "Basic cleaning...\n",
      "   Dropping null values\n",
      "   Getting rid of punctuation and HTML tags\n",
      "                                                  Reviews  Sentiments\n",
      "0       a series of escapades demonstrating the adage ...           1\n",
      "1       a series of escapades demonstrating the adage ...           2\n",
      "2                                                a series           2\n",
      "3                                                       a           2\n",
      "4                                                  series           2\n",
      "...                                                   ...         ...\n",
      "156055                                           hearst s           2\n",
      "156056                          forced avuncular chortles           1\n",
      "156057                                 avuncular chortles           3\n",
      "156058                                          avuncular           2\n",
      "156059                                           chortles           2\n",
      "\n",
      "[156060 rows x 2 columns]\n",
      "NLP cleaning...\n",
      "   Removing stop words\n",
      "   Lemmatizing reviews\n",
      "                                                  Reviews  Sentiments\n",
      "0       series escapade demonstrating adage good goose...           1\n",
      "1          series escapade demonstrating adage good goose           2\n",
      "2                                                  series           2\n",
      "3                                                                   2\n",
      "4                                                  series           2\n",
      "...                                                   ...         ...\n",
      "156055                                           hearst s           2\n",
      "156056                           forced avuncular chortle           1\n",
      "156057                                  avuncular chortle           3\n",
      "156058                                          avuncular           2\n",
      "156059                                            chortle           2\n",
      "\n",
      "[156060 rows x 2 columns]\n",
      "   Vecorizing full training data : tfidf\n",
      "2\n",
      "******************* PREPROCESSING TEST DATA *******************\n",
      "Reading input data\n",
      "(66292, 1)\n",
      "                                                    Reviews\n",
      "PhraseId                                                   \n",
      "156061    An intermittently pleasing but mostly routine ...\n",
      "156062    An intermittently pleasing but mostly routine ...\n",
      "156063                                                   An\n",
      "156064    intermittently pleasing but mostly routine effort\n",
      "156065           intermittently pleasing but mostly routine\n",
      "Basic cleaning...\n",
      "   Dropping null values\n",
      "   Getting rid of punctuation and HTML tags\n",
      "NLP cleaning...\n",
      "   Removing stop words\n",
      "   Lemmatizing reviews\n",
      "   Vecorizing test data : tfidf\n"
     ]
    }
   ],
   "source": [
    "print(\"******************* PREPROCESSING TRAIN DATA *******************\")\n",
    "\n",
    "train_vec, train_data_list = data_preprocessor_train(\"train.tsv\", '\\t', 0, [2, 3], {\"Phrase\":\"Reviews\",\"Sentiment\":\"Sentiments\"})\n",
    "# print(train_vec)\n",
    "# print(len(train_data_list))\n",
    "# for i in range(4):\n",
    "#     print(train_data_list[i].shape)\n",
    "\n",
    "print(\"******************* PREPROCESSING FULL TRAIN DATA *******************\")\n",
    "\n",
    "# preprocess full train dataset to use for final prediction with best model found in the benchmark phase\n",
    "full_train_vec, full_train_data_list = data_preprocessor_full_train(\"train.tsv\", '\\t', 0, [2, 3], {\"Phrase\":\"Reviews\",\"Sentiment\":\"Sentiments\"})\n",
    "    \n",
    "print(\"******************* PREPROCESSING TEST DATA *******************\")\n",
    "# CHANGE THE VEC !!!\n",
    "test_vec, test_data = data_preprocessor_test(\"test.tsv\", '\\t', 0, [0, 2], [0],{\"Phrase\":\"Reviews\"}, full_train_vec) #NO NEED FOR THE PHRASEID COLUMN (INDEX IS NOT NECESSARY)\n",
    "# #print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class ML_Model:\n",
    "\n",
    "    def __init__(self,model_name, model, train_data_list, test_data, best_c, save) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.X_train, self.y_train, self.X_eval, self.y_eval = train_data_list\n",
    "        self.X_test = test_data\n",
    "        assert type(self.X_test) == scipy.sparse.csr.csr_matrix\n",
    "        self.best_c = best_c\n",
    "        self.save = save\n",
    "        self.trained_model = False\n",
    "\n",
    "    def load_model(self) -> None:\n",
    "        print(\"Trying to load model\")\n",
    "        try:\n",
    "            file = open(f\"model_{self.model_name}.pkl\",\"rb\")\n",
    "            self.model = pickle.load(file)\n",
    "            file.close()\n",
    "            print(\"Trained model found !\")\n",
    "            self.trained_model = True\n",
    "        except:\n",
    "            print(\"No trained model available\")\n",
    "            \n",
    "    \n",
    "    def train_model(self) -> None:\n",
    "        print(\"Training model...\")\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        acc = accuracy_score(self.y_eval, self.model.predict(self.X_eval))\n",
    "        print(self.model_name, acc)\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"Saving model\")\n",
    "        filehandler = open(f\"model_{self.model_name}.pkl\",\"wb\")\n",
    "        pickle.dump(self.model,filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "    def predict(self):\n",
    "        y_test =  self.model.predict(self.X_test)\n",
    "        return y_test\n",
    "        \n",
    "def main(model_name, model, train_data_list, test_data, best_c = None, save = True, pred = True):\n",
    "    ml_model = ML_Model(model_name, model, train_data_list, test_data, best_c, save)\n",
    "    ml_model.load_model()\n",
    "    if ml_model.trained_model == False:\n",
    "        ml_model.train_model()\n",
    "        if save:\n",
    "            ml_model.save_model()\n",
    "    if pred :\n",
    "        y_pred = ml_model.predict()\n",
    "        submission = pd.DataFrame({\"PhraseId\":[i for i in range(156061,222353)],\"Sentiment\":y_pred})\n",
    "        submission.to_csv(f\"Submission_{model_name}.csv\", index=False)\n",
    "        return y_pred\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************___svc___***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25}\n",
      "0.7460226533003516\n",
      "***************___lr___***************\n",
      "{'C': 25}\n",
      "0.7478453326389793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model_dict = {\"svc\" : LinearSVC(max_iter=1000, random_state=0),\n",
    "            \"lr\" : LogisticRegression(random_state=0,multi_class=\"multinomial\", solver='saga', n_jobs = -1, max_iter=1000)}\n",
    "            #\"knc\" : KNeighborsClassifier()\n",
    "  #\"random_forest\":RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\"naive_bayes\" : MultinomialNB(),\n",
    "\n",
    "params_dict = {\"svc\" : {'C':[0.01,0.1,1,2,5,10,25]}, \n",
    "            \"lr\" : {'C':[0.01,0.1,1,2,5,10,25]}}\n",
    "           # \"knc\" : {'n_neighbors':[3,5,10], 'weights':['uniform','distance']}} #'penalty': ['l1', 'l2']\n",
    "\n",
    "def benchmark(train_data_list, test_data):\n",
    "    X_train = train_data_list[0]\n",
    "    y_train = train_data_list[1]\n",
    "    X_eval = train_data_list[2]\n",
    "    y_eval = train_data_list[3]\n",
    "    X_test = test_data\n",
    "    for model_item in model_dict.items():\n",
    "        model_name = model_item[0]\n",
    "        print(f\"***************___{model_name}___***************\")\n",
    "        model = model_item[1]\n",
    "        params = params_dict[model_name]\n",
    "        grid_clf_acc = GridSearchCV(model, param_grid = params,scoring = 'accuracy')\n",
    "        grid_clf_acc.fit(X_train, y_train)\n",
    "        print(grid_clf_acc.best_params_)\n",
    "        y_eval_pred = grid_clf_acc.predict(X_eval)\n",
    "        acc = accuracy_score(y_eval,y_eval_pred)\n",
    "        print(acc)\n",
    "        y_pred = grid_clf_acc.predict(X_test)\n",
    "        submission = pd.DataFrame({\"PhraseId\":[i for i in range(156061,222353)],\"Sentiment\":y_pred})\n",
    "        submission.to_csv(f\"Submission_{model_name}.csv\", index=False)\n",
    "        # print(classification_report(y_test,predic))\n",
    "        # print(confusion_matrix(y_test, predic))\n",
    "\n",
    "benchmark(train_data_list,test_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0,multi_class=\"multinomial\", solver='saga', C= 2, n_jobs = -1, max_iter=1000)\n",
    "X_train_full = full_train_data_list[0]\n",
    "y_train_full = full_train_data_list[1]\n",
    "model.fit(X_train_full,y_train_full)\n",
    "y_pred = model.predict(test_data)\n",
    "submission = pd.DataFrame({\"PhraseId\":[i for i in range(156061,222353)],\"Sentiment\":y_pred})\n",
    "submission.to_csv(f\"Submission_lr_full_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89dc794d539d73779b3fe852555a2ba6a47713f109fb082adb0c727d8b90d0fa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sentiment': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
