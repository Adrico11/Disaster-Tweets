{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CURRENT STANDINGS : 330/860 !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KAGGLE COMPETITION : MACHINE LEARNING APPROACH**\n",
    "\n",
    "**A. Preprocessing**\n",
    "1. Import libraries\n",
    "2. Import dataset\n",
    "3. Basic data cleaning : get rid of null values, punctuation and html tags\n",
    "4. NLP data preprocessing : remove stop words, stem/lemmatize words (+ remove repeating words)\n",
    "5. Test-Train split\n",
    "6. Vecorize data (fit on the train data and use on train+test data) with countvec or tfidfvec\n",
    "7. Oversample data (deal with imbalanced data with SMOTE)\n",
    "\n",
    "**B. Models**\n",
    "Benchmark ML models (LogisticRegression, LinearSVC, Multi-class Naive Bayes, Random forest classifier...)\n",
    "\n",
    "**C. Make prediction** \n",
    "\n",
    "Submit predictions and put my name on the leaderboard !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adrie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import os\n",
    "import nltk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "def clean_reviews(review):\n",
    "    #find and replace all punctuation with \"\" and all html tags with \" \" + lower\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    review = REPLACE_NO_SPACE.sub(\"\", review.lower()) #replace specified characters with \"\" in the review.lower() text\n",
    "    review = REPLACE_WITH_SPACE.sub(\" \", review)\n",
    "    return review\n",
    "\n",
    "# def remove_repeating_words(review):\n",
    "#     review1 = yield(gensim.utils.simple_preprocess(str(review),deacc=True))\n",
    "#     return review1\n",
    "\n",
    "def remove_stop_words(review):\n",
    "    stop_words_list = ENGLISH_STOP_WORDS #stop_words.\n",
    "    review = \" \".join(word for word in review.split(\" \") if word not in stop_words_list)\n",
    "    return review\n",
    "\n",
    "def stem_reviews(review):\n",
    "    # PB : NEED TO INITIATE A NEW STEMMER FOR EACH ROW...\n",
    "    stemmer = PorterStemmer()\n",
    "    #separator.join(list of strings) => 1 long string\n",
    "    #string.split(separator) => list of strings\n",
    "    review = \" \".join([stemmer.stem(word) for word in review.split(\" \")])\n",
    "    return review\n",
    "\n",
    "def lemmatize_reviews(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #separator.join(list of strings) => 1 long string\n",
    "    #string.split(separator) => list of strings\n",
    "    review = \" \".join([lemmatizer.lemmatize(word) for word in review.split(\" \")])\n",
    "    return review\n",
    "\n",
    "def oversample(data_list):\n",
    "    [X_train, y_train, X_eval, y_eval] = data_list\n",
    "    sm = SMOTE()\n",
    "    X_train_sm, y_train_sm = sm.fit_resample(X_train,y_train)\n",
    "    X_eval_sm, y_eval_sm = sm.fit_resample(X_eval,y_eval)\n",
    "    assert len(y_train_sm.value_counts().unique())==1 and len(y_eval_sm.value_counts().unique())==1\n",
    "    return [X_train_sm, y_train_sm,X_eval_sm, y_eval_sm]\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, df, train = True, vec = None, nlp_method = \"lemmatize\", vec_name = \"tfidf\", stop_words = \"english\", ngram_range = (1,2)) -> None: # change vec_name to tfidf to see what changes !!!\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "        self.vec = vec\n",
    "        assert (self.train == True and self.vec == None) or (self.train == False and self.vec != None)\n",
    "        assert nlp_method in [\"None\", \"stem\",\"lemmatize\"]\n",
    "        self.nlp_method = nlp_method\n",
    "        assert vec_name in [\"binary\",\"count\",\"tfidf\"]\n",
    "        self.vec_name = vec_name\n",
    "        self.stop_words = stop_words # USELESS IF REMOVED BEFORE !!! (in the basic cleaning...)\n",
    "        self.ngram_range = ngram_range\n",
    "        if self.train == True : #same as self.vec == None\n",
    "            if self.vec_name == \"tfidf\":\n",
    "                self.vec =  TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=self.ngram_range) #stop_words=self.stop_words,\n",
    "            else:\n",
    "                self.vec = CountVectorizer(binary=self.vec_name == \"binary\", ngram_range=self.ngram_range) # stop_words=self.stop_words,\n",
    "        else:\n",
    "            self.vec = vec\n",
    "        self.train_df = self.test_df = None\n",
    "\n",
    "    def basic_clean(self) -> None:\n",
    "        print(\"   Dropping null values\")\n",
    "        self.df.dropna(inplace = True)\n",
    "        print(\"   Getting rid of punctuation and HTML tags\")\n",
    "        self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : clean_reviews(row))\n",
    "        print(\"   Removing repeating words\")\n",
    "        \n",
    "        # if self.train: #no labels in the test data\n",
    "        #     self.df[\"Sentiments\"] = self.df[\"Sentiments\"].apply(lambda row : 0 if row==\"negative\" else 1)\n",
    "\n",
    "    def nlp_clean(self) -> None:\n",
    "        print(\"   Removing stop words\")\n",
    "        self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : remove_stop_words(row))\n",
    "        if self.nlp_method == \"stem\":\n",
    "            print(\"   Stemming reviews\")\n",
    "            self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : stem_reviews(row))\n",
    "        elif self.nlp_method == \"lemmatize\":\n",
    "            print(\"   Lemmatizing reviews\")\n",
    "            self.df[\"Reviews\"] = self.df[\"Reviews\"].apply(lambda row : lemmatize_reviews(row))\n",
    "        else:\n",
    "            print(\"   No NLP cleaning\")\n",
    "\n",
    "    def train_eval_split(self, train_size = 0.8, random_state = 42) -> None:\n",
    "        M = self.df.shape[0]\n",
    "        self.train_df = self.df.sample(n=int(train_size*M), axis=0, random_state = random_state, ignore_index=True)\n",
    "        self.eval_df = self.df[~self.df.index.isin(self.train_df.index)].reset_index(drop=True)\n",
    "\n",
    "    def vectorize_train(self):\n",
    "        print(f\"   Vecorizing training data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        train_reviews = self.train_df[\"Reviews\"]\n",
    "        eval_reviews = self.eval_df[\"Reviews\"]\n",
    "        vec.fit(train_reviews)\n",
    "        X_train = vec.transform(train_reviews)\n",
    "        X_eval = vec.transform(eval_reviews)\n",
    "        return vec, X_train, X_eval\n",
    "\n",
    "    def final_split(self):\n",
    "        y_train = self.train_df[\"Sentiments\"]\n",
    "        y_eval = self.eval_df[\"Sentiments\"]\n",
    "        vec, X_train, X_eval = self.vectorize_train()\n",
    "        return vec, [X_train, y_train, X_eval, y_eval]\n",
    "\n",
    "    def vectorize_test(self):\n",
    "        print(f\"   Vecorizing test data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        vec_reviews = vec.transform(self.df[\"Reviews\"])\n",
    "        #full_vec_reviews = PUT EVERYTHING BACK TOGETHER ???\n",
    "        return vec, vec_reviews\n",
    "\n",
    "    def vectorize_full_train(self):\n",
    "        print(f\"   Vecorizing full training data : {self.vec_name}\")\n",
    "        vec = self.vec\n",
    "        X_train_full = vec.fit_transform(self.df[\"Reviews\"])\n",
    "        y_train_full = self.df[\"Sentiments\"]\n",
    "        #full_vec_reviews = PUT EVERYTHING BACK TOGETHER ???\n",
    "        return vec, [X_train_full, y_train_full]\n",
    "\n",
    "def data_preprocessor_train(filename, separator, header, use_cols, col_dict, smote = True):\n",
    "    \"\"\"Preprocess the train dataset (subset of the initial train dataset) to train the ML models in the benchmark phase\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator, usecols = use_cols, header = header)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    print(\"Train-Eval split...\")\n",
    "    df_preprocess.train_eval_split()\n",
    "    train_vec, train_data_list = df_preprocess.final_split()\n",
    "    if smote:\n",
    "        print(\"Balancing dataset...\")\n",
    "        train_data_list = oversample(train_data_list)\n",
    "    return train_vec, train_data_list  #vec, X_train, y_train, X_eval, y_eval\n",
    "\n",
    "def data_preprocessor_test(filename, separator, header, use_cols, col_dict, vec): #index_col\n",
    "    \"\"\"Preprocess the eval dataset (subset of the initial train dataset) to evaluate the ML models in the benchmark phase\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator,header = header, usecols = use_cols) #, index_col = index_col)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init, train = False, vec = vec)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    test_vec, test_data_list = df_preprocess.vectorize_test()\n",
    "    return test_vec, test_data_list  #vec, X_test, y_test\n",
    "\n",
    "def data_preprocessor_full_train(filename, separator, header, use_cols, col_dict, smote = True):\n",
    "    \"\"\"Preprocess the whole train dataset to make better final predictions\"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    df_init = pd.read_csv(filename, sep = separator, usecols = use_cols, header = header)\n",
    "    df_init.rename(columns = col_dict, inplace = True)\n",
    "    print(df_init.shape)\n",
    "    print(df_init.head())\n",
    "    df_preprocess = Preprocessor(df = df_init)\n",
    "    print(\"Basic cleaning...\")\n",
    "    df_preprocess.basic_clean()\n",
    "    print(\"NLP cleaning...\")\n",
    "    df_preprocess.nlp_clean()\n",
    "    full_train_vec, full_train_data_list = df_preprocess.vectorize_full_train()\n",
    "    # if smote:\n",
    "    #     print(\"Balancing dataset...\")\n",
    "    #     full_train_data_list = oversample(full_train_data_list)\n",
    "    return full_train_vec, full_train_data_list  #vec, X_train, y_train, X_eval, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* PREPROCESSING FULL TRAIN DATA *******************\n",
      "Reading input data\n",
      "(7613, 2)\n",
      "                                             Reviews  Sentiments\n",
      "0  Our Deeds are the Reason of this #earthquake M...           1\n",
      "1             Forest fire near La Ronge Sask. Canada           1\n",
      "2  All residents asked to 'shelter in place' are ...           1\n",
      "3  13,000 people receive #wildfires evacuation or...           1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...           1\n",
      "Basic cleaning...\n",
      "   Dropping null values\n",
      "   Getting rid of punctuation and HTML tags\n",
      "   Removing repeating words\n",
      "NLP cleaning...\n",
      "   Removing stop words\n",
      "   Lemmatizing reviews\n",
      "   Vecorizing full training data : tfidf\n",
      "(7613, 3576)\n",
      "(7613,)\n",
      "******************* PREPROCESSING TEST DATA *******************\n",
      "Reading input data\n",
      "(3263, 1)\n",
      "                                             Reviews\n",
      "0                 Just happened a terrible car crash\n",
      "1  Heard about #earthquake is different cities, s...\n",
      "2  there is a forest fire at spot pond, geese are...\n",
      "3           Apocalypse lighting. #Spokane #wildfires\n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan\n",
      "Basic cleaning...\n",
      "   Dropping null values\n",
      "   Getting rid of punctuation and HTML tags\n",
      "   Removing repeating words\n",
      "NLP cleaning...\n",
      "   Removing stop words\n",
      "   Lemmatizing reviews\n",
      "   Vecorizing test data : tfidf\n",
      "(3263, 3576)\n"
     ]
    }
   ],
   "source": [
    "# print(\"******************* PREPROCESSING TRAIN DATA *******************\")\n",
    "\n",
    "# train_vec, train_data_list = data_preprocessor_train(\"data/train.csv\",\",\", 0, [3, 4], {\"text\":\"Reviews\",\"target\":\"Sentiments\"}, False)\n",
    "# # print(train_data_list)\n",
    "# # print(train_vec)\n",
    "# # print(len(train_data_list))\n",
    "# for i in range(4):\n",
    "#     print(train_data_list[i].shape)\n",
    "\n",
    "print(\"******************* PREPROCESSING FULL TRAIN DATA *******************\")\n",
    "\n",
    "# preprocess full train dataset to use for final prediction with best model found in the benchmark phase\n",
    "full_train_vec, full_train_data_list = data_preprocessor_full_train(\"data/train.csv\",\",\", 0, [3, 4], {\"text\":\"Reviews\",\"target\":\"Sentiments\"}, False)\n",
    "for i in range(2):\n",
    "    print(full_train_data_list[i].shape)\n",
    "\n",
    "print(\"******************* PREPROCESSING TEST DATA *******************\")\n",
    "# CHANGE THE VEC !!!\n",
    "test_vec, test_data = data_preprocessor_test(\"data/test.csv\", ',', 0, [3], {\"text\":\"Reviews\"}, full_train_vec) #NO NEED FOR THE PHRASEID COLUMN (INDEX IS NOT NECESSARY) [0]\n",
    "print(test_data.shape)\n",
    "# #print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************___svc___***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\svm\\_base.py:249: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'kernel': 'linear'}\n",
      "***************___lr___***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5, 'penalty': 'l2'}\n",
      "***************___knc___***************\n",
      "{'n_neighbors': 5, 'weights': 'distance'}\n",
      "***************___random_forest___***************\n",
      "{'max_depth': 5, 'n_estimators': 100}\n",
      "***************___naive_bayes___***************\n",
      "{'alpha': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\adrie\\anaconda3\\envs\\sentiment\\lib\\site-packages\\sklearn\\naive_bayes.py:512: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model_dict = {\"svc\" : SVC(max_iter=1000, random_state=0),\n",
    "            \"lr\" : LogisticRegression(random_state=0, solver='liblinear', n_jobs = -1, max_iter=10000),\n",
    "            \"knc\" : KNeighborsClassifier(),\n",
    "            \"random_forest\":RandomForestClassifier(random_state=0, n_jobs=-1),\n",
    "            \"naive_bayes\" : MultinomialNB()}\n",
    "\n",
    "params_dict = {\"svc\" : {'C':[0.01,0.1,1,2,5,10,25], 'kernel':['linear', 'rbf', 'poly']}, \n",
    "            \"lr\" : {'C':[0.01,0.1,1,2,5,10,25], 'penalty':['l1','l2']},\n",
    "            \"knc\" : {'n_neighbors':[3,5,10], 'weights':['uniform','distance']},\n",
    "            \"random_forest\":{'n_estimators':[100,500,1000], 'max_depth':[2,3,5]}, #'penalty': ['l1', 'l2']\n",
    "            \"naive_bayes\" : {'alpha':[1,2]}}\n",
    "\n",
    "# from sklearn.metrics import f1_score, make_scorer\n",
    "# f1 = make_scorer(f1_score , average='macro')\n",
    "\n",
    "# def benchmark(train_data_list, test_data):\n",
    "#     X_train = train_data_list[0]\n",
    "#     y_train = train_data_list[1]\n",
    "#     X_eval = train_data_list[2]\n",
    "#     y_eval = train_data_list[3]\n",
    "#     X_test = test_data\n",
    "#     for model_item in model_dict.items():\n",
    "#         model_name = model_item[0]\n",
    "#         print(f\"***************___{model_name}___***************\")\n",
    "#         model = model_item[1]\n",
    "#         params = params_dict[model_name]\n",
    "#         grid_clf_acc = GridSearchCV(model, param_grid = params,scoring = 'f1')\n",
    "#         grid_clf_acc.fit(X_train, y_train)\n",
    "#         print(grid_clf_acc.best_params_)\n",
    "#         y_eval_pred = grid_clf_acc.predict(X_eval)\n",
    "#         f1 = f1_score(y_eval,y_eval_pred)\n",
    "#         print(f1)\n",
    "#         print(classification_report(y_eval,y_eval_pred))\n",
    "#         print(confusion_matrix(y_eval,y_eval_pred))\n",
    "#         y_pred = grid_clf_acc.predict(X_test)\n",
    "#         submission = pd.read_csv(\"data/test.csv\",usecols=[0])\n",
    "#         submission[\"target\"] = y_pred  #pd.DataFrame({\"id\":[i for i in range(156061,222353)],\"target\":y_pred})\n",
    "#         submission.to_csv(f\"Submission_{model_name}.csv\", index=False)\n",
    "\n",
    "def benchmark_full_train(full_train_data_list, test_data):\n",
    "    X_train = full_train_data_list[0]\n",
    "    y_train = full_train_data_list[1]\n",
    "    X_test = test_data\n",
    "    for model_item in model_dict.items():\n",
    "        model_name = model_item[0]\n",
    "        print(f\"***************___{model_name}___***************\")\n",
    "        model = model_item[1]\n",
    "        params = params_dict[model_name]\n",
    "        grid_clf_acc = GridSearchCV(model, param_grid = params,scoring = 'f1')\n",
    "        grid_clf_acc.fit(X_train, y_train)\n",
    "        print(grid_clf_acc.best_params_)\n",
    "        y_pred = grid_clf_acc.predict(X_test)\n",
    "        submission = pd.read_csv(\"data/test.csv\",usecols=[0])\n",
    "        submission[\"target\"] = y_pred  #pd.DataFrame({\"id\":[i for i in range(156061,222353)],\"target\":y_pred})\n",
    "        submission.to_csv(f\"Submission_{model_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "benchmark_full_train(full_train_data_list,test_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(random_state=0,multi_class=\"multinomial\", solver='saga', C= 2, n_jobs = -1, max_iter=1000)\n",
    "# X_train_full = full_train_data_list[0]\n",
    "# y_train_full = full_train_data_list[1]\n",
    "# model.fit(X_train_full,y_train_full)\n",
    "# y_pred = model.predict(test_data)\n",
    "# submission = pd.DataFrame({\"PhraseId\":[i for i in range(156061,222353)],\"Sentiment\":y_pred})\n",
    "# submission.to_csv(f\"Submission_lr_full_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89dc794d539d73779b3fe852555a2ba6a47713f109fb082adb0c727d8b90d0fa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sentiment': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
