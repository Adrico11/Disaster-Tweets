{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8InucvJnt_LS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6blXGnWt_LT"
      },
      "source": [
        "**1. Preprocess datasets**\n",
        "\n",
        "- Import BERT Tokenizer + add tokens to mask URLs and usernames\n",
        "- Basic data preprocessing : get rid of tags, links and usernames\n",
        "- Bert preprocessing : tokenize, create inputs and attention masks\n",
        "- Form train and test datasets (in the correct format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aV0NGbG5t_LU"
      },
      "outputs": [],
      "source": [
        "def define_tokenizer():\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    ## Adding additional tokens for masking URLs and usernames in tweets\n",
        "    bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\n",
        "    return bert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qs24m1tEt_LV"
      },
      "outputs": [],
      "source": [
        "def bert_tokenize(df, tokenizer, max_seq_len = 100):\n",
        "    input_sequences = []\n",
        "    # The attention mask is an optional argument used when batching sequences together.\n",
        "    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n",
        "    attention_masks = []\n",
        "    bert_text = []\n",
        "    \n",
        "    # some very minor text processing (try to keep the text as close as original)\n",
        "    for i, text in enumerate(df['text']):\n",
        "#         print(i, text)\n",
        "        text = text.replace(\"\\n\", \" \").split(\" \")\n",
        "        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n",
        "        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n",
        "        text = \" \".join(text)\n",
        "        text = re.sub(r'#', '', text)\n",
        "        bert_text.append(text)\n",
        "        \n",
        "#         print(i, text)\n",
        "        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n",
        "        input_ids = sequence_dict['input_ids']\n",
        "        att_mask = sequence_dict['attention_mask']\n",
        "#         print(i, tokenizer.tokenize(text))\n",
        "        input_sequences.append(input_ids)\n",
        "        attention_masks.append(att_mask)\n",
        "    \n",
        "    df['bert_text'] = bert_text\n",
        "    return input_sequences, attention_masks, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjQYsGdTt_LW"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_df):\n",
        "\n",
        "    bert_tokenizer = define_tokenizer()\n",
        "    train_X, train_att, train_df = bert_tokenize(train_df,bert_tokenizer)\n",
        "    train_y = train_df['target'].values\n",
        "\n",
        "    # Checking the tokenized format\n",
        "    # print(train_X[0])\n",
        "    # print(train_att[0])\n",
        "\n",
        "    return train_X, train_att,train_y\n",
        "\n",
        "def create_train_dataloader(train_X, train_att,train_y,batch_size = 64):\n",
        "\n",
        "    ## Forming the datasets\n",
        "    train_X = torch.tensor(train_X)\n",
        "    train_y = torch.tensor(train_y)\n",
        "    train_att = torch.tensor(train_att)\n",
        "\n",
        "    train_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_data)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def create_dataloaders(train_idx, val_idx, train_X, train_att,train_y,batch_size = 64):\n",
        "\n",
        "    ## Forming the datasets\n",
        "    train_X = torch.tensor(train_X)\n",
        "    train_y = torch.tensor(train_y)\n",
        "    train_att = torch.tensor(train_att)\n",
        "\n",
        "    train_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler) #train_X ??\n",
        "    valid_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "    # train_sampler = torch.utils.data.RandomSampler(train_data)\n",
        "    # train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q0lxjPFt_LX"
      },
      "source": [
        "**2. Adapt and Train Bert model**\n",
        "\n",
        "- Import pre-trained BERT model for classification\n",
        "- Resize token embeddings (since we have added two special ones)\n",
        "- Define train and test functions\n",
        "- Train model and show metrics with Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KapYjbk_t_LY"
      },
      "outputs": [],
      "source": [
        "def define_model(bert_tokenizer, show = False):\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "    model.resize_token_embeddings(len(bert_tokenizer))\n",
        "\n",
        "    if show :\n",
        "      # Print model's state_dict\n",
        "      print(\"Model's state_dict:\")\n",
        "      for param_tensor in model.state_dict():\n",
        "          print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "    # select device    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    IPython.display.clear_output()\n",
        "    \n",
        "    return model, device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ompc21NDUQ3s"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "model_name = 'network'\n",
        "log_name = '{}_{}'.format(model_name, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
        "writer = SummaryWriter('logs/{}'.format(log_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MT6hJ0VWt_LZ"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\n",
        "def train(BERT_model, epoch, device, train_dataloader):\n",
        "\n",
        "    # running_accuracy = 0.0\n",
        "    running_loss = 0.0\n",
        "    running_f1 = 0.0\n",
        "\n",
        "    # define model optimizer and loss function\n",
        "\n",
        "    optimizer = AdamW(BERT_model.parameters(), lr=2e-5, eps=1e-8)\n",
        "    loss_fct = torch.nn.NLLLoss()\n",
        "\n",
        "    t0 = datetime.now()\n",
        "    BERT_model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader, start=1):\n",
        "\n",
        "        # get the inputs : batch is a list of [inputs, att_masks, labels]\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, att_masks, labels = batch\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        BERT_model.zero_grad()  \n",
        "        \n",
        "        # forward propagation\n",
        "        logits = BERT_model(inputs, attention_mask=att_masks)\n",
        "        outputs = F.log_softmax(logits[0], dim=1)\n",
        "        \n",
        "        # compute loss function + backward propagation\n",
        "        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        # updating current accuracy and f1 scores (after batch i)\n",
        "        pred_outputs = outputs.detach().cpu().numpy()\n",
        "        pred = np.argmax(pred_outputs, axis=1)\n",
        "        labels = labels.cpu().numpy()\n",
        "        # current_acc = accuracy_score(pred, labels)\n",
        "        # running_accuracy += current_acc\n",
        "        current_f1 = f1_score(pred,labels)\n",
        "        running_f1 += current_f1\n",
        "        \n",
        "        # optimize parameters\n",
        "        torch.nn.utils.clip_grad_norm_(BERT_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i % 20 == 0: # every 20 batches\n",
        "\n",
        "            nb_batches = len(train_dataloader)\n",
        "            nb_samples = len(train_dataloader.dataset)\n",
        "            batch_size = len(inputs)\n",
        "            current_batch = epoch*nb_batches + i\n",
        "\n",
        "            # add the current metrics to the Tensorboard\n",
        "            writer.add_scalar('training loss', running_loss/20, current_batch)\n",
        "            # writer.add_scalar('training accuracy', running_accuracy/20, current_batch)\n",
        "            writer.add_scalar('training f1', running_f1/20, current_batch)\n",
        "\n",
        "            print('Train Epoch: {} [{}/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}  | F1: {:.4f}  |  Accuracy: {:.4f}'.format(\n",
        "                epoch, i*batch_size, nb_samples,\n",
        "                    i / nb_batches, datetime.now() - t0, loss.item(), \n",
        "                    current_f1\n",
        "            ))\n",
        "\n",
        "            # running_accuracy = 0.0\n",
        "            running_loss = 0.0\n",
        "            running_f1 = 0.0\n",
        "\n",
        "\n",
        "    return BERT_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TeXwGUD1PZ07"
      },
      "outputs": [],
      "source": [
        "def valid(BERT_model, epoch, device, valid_dataloader):\n",
        "\n",
        "    t0 = datetime.now()\n",
        "    loss_fct = torch.nn.NLLLoss()\n",
        "    running_val_loss = 0.0\n",
        "    running_val_f1 = 0.0\n",
        "    BERT_model.eval()\n",
        "\n",
        "    for i, batch in enumerate(valid_dataloader, start=1):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, att_masks, labels = batch\n",
        "        with torch.no_grad():\n",
        "            logits = BERT_model(inputs, attention_mask=att_masks)\n",
        "            outputs = F.log_softmax(logits[0], dim=1)\n",
        "            \n",
        "            loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        running_val_loss += loss.item()\n",
        "        pref_outputs = outputs.detach().cpu().numpy()\n",
        "        pred = np.argmax(pref_outputs, axis=1)\n",
        "        labels = labels.cpu().numpy()\n",
        "        current_val_f1 = f1_score(pred,labels)\n",
        "        running_val_f1 += current_val_f1\n",
        "\n",
        "        if i % 20 == 0: # every 20 batches\n",
        "\n",
        "            nb_batches = len(valid_dataloader)\n",
        "            nb_samples = len(valid_dataloader.dataset)\n",
        "            batch_size = len(inputs)\n",
        "            current_batch = epoch*nb_batches + i\n",
        "\n",
        "            # add the current metrics to the Tensorboard\n",
        "            writer.add_scalar('validation loss', running_val_loss/20, current_batch)\n",
        "            # writer.add_scalar('training accuracy', running_accuracy/20, current_batch)\n",
        "            writer.add_scalar('validation f1', running_val_f1/20, current_batch)\n",
        "\n",
        "            print('Valid set: Loss: {:.4f}  | F1: {:.4f}'.format(\n",
        "                loss.item(), current_val_f1\n",
        "            ))\n",
        "\n",
        "            # running_accuracy = 0.0\n",
        "            running_val_loss = 0.0\n",
        "            running_val_f1 = 0.0\n",
        "\n",
        "    # test_loss /= len(test_dataloader)\n",
        "    # test_acc /= len(test_dataloader)\n",
        "    # print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n",
        "    #     test_loss, test_acc, datetime.now() - t0\n",
        "    # ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BB57Iy9bt_La"
      },
      "outputs": [],
      "source": [
        "# from torchsummary import summary\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def train_model(BERT_model, device, train_data, k = 10, nb_epoch = 1, save = True, show_summary = True):\n",
        "\n",
        "    t0 = datetime.now()\n",
        "    train_X, train_att,train_y = train_data\n",
        "    splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
        "\n",
        "\n",
        "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_X)))): #train_X.shape[0]\n",
        "        print(f\"FOLD NUMBER {fold}\")\n",
        "        train_dataloader, valid_dataloader = create_dataloaders(train_idx, val_idx, train_X, train_att,train_y)\n",
        "\n",
        "        for epoch in range(1, nb_epoch+1): # loop over the dataset multiple times\n",
        "            print(f\"##### Training Epoch {epoch} #####\")\n",
        "            trained_model = train(BERT_model, epoch, device, train_dataloader)\n",
        "            valid(BERT_model, epoch, device, valid_dataloader)\n",
        "\n",
        "        total_time = round(((datetime.now()-t0).seconds)/60,2) #total training time in minutes\n",
        "        print(f\"Total training time for {nb_epoch} epochs : {total_time} minutes\")\n",
        "\n",
        "    if save:\n",
        "        torch.save(trained_model.state_dict(), \"trained_model.pt\")\n",
        "\n",
        "    return trained_model\n",
        "\n",
        "    # if show_summary:\n",
        "    #     summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z5-J9khc9WyL"
      },
      "outputs": [],
      "source": [
        "def main(train_df):\n",
        "    print(\"Defining Tokenizer\")\n",
        "    bert_tokenizer = define_tokenizer()\n",
        "    print(\"Defining Model\")\n",
        "    base_model, device = define_model(bert_tokenizer)\n",
        "    print(\"Preprocessing dataset\")\n",
        "    train_data = preprocess_data(train_df)\n",
        "    print(\"Training model\")\n",
        "    train_model(base_model,device, train_data) #, train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-c448b1acf00584fe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-c448b1acf00584fe\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "UBktvY7iKVrl",
        "outputId": "5bf3c504-502f-4cc1-c94f-ece113f5ac77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "C:\\Users\\adrie\\anaconda3\\envs\\tranformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD NUMBER 0\n",
            "##### Training Epoch 1 #####\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "main(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sqEjnVwt_La"
      },
      "source": [
        "**3. Predict on Test data**\n",
        "\n",
        "- Define predict function\n",
        "- Generate submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPqiMgBTt_La"
      },
      "outputs": [],
      "source": [
        "def predict(corpus):\n",
        "\n",
        "    # initialize the BERT model\n",
        "    bert_tokenizer = define_tokenizer()\n",
        "    the_model, device = define_model(bert_tokenizer)\n",
        "    # the_model = define_model()\n",
        "\n",
        "    #try to load a previsouly trained model or train a new one \n",
        "    try:\n",
        "      print(\"Trying to load a previously trained model\")\n",
        "      the_model.load_state_dict(torch.load(\"trained_model\"))\n",
        "\n",
        "    except :\n",
        "      print(\"Training a new model\")\n",
        "      the_model = train_model(the_model)\n",
        "    \n",
        "    print(\"Making predictions !\")\n",
        "    predictions = []\n",
        "    for text in corpus:\n",
        "        # pre-process text\n",
        "        input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n",
        "        logits = the_model.eval()(input_ids=input_)[0]\n",
        "        prob = F.softmax(logits, dim=1)[0]\n",
        "        pred = np.argmax(prob.cpu().detach().numpy())\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_AzxYPSt_Lb"
      },
      "outputs": [],
      "source": [
        "def make_predictions(test_df):\n",
        "  \"\"\"Generate submission file to upload on Kaggle\"\"\"\n",
        "  predictions = predict(test_df.text)\n",
        "  sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
        "  sample_submission[\"target\"] = predictions\n",
        "  pd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)\n",
        "  sample_submission.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmn0JtZyutSn"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "make_predictions(test_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Kaggle Bert 1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bd7f66c493c3378b00f1c25d794bdfda53f41f0227b6475fe6cc86775751149d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('tranformers')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
